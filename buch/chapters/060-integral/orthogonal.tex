%
% orthogonal.tex
%
% (c) 2021 Prof Dr Andreas Müller, OST Ostschweizer Fachhochschule
%
\section{Orthogonalität
\label{buch:integral:section:orthogonale-polynome}}
\rhead{Orthogonale Polynome}
Die Fourier-Theorie basiert auf der Idee, Funktionen durch 
Funktionenreihen mit Summanden zu bilden, die im Sinne eines
Skalarproduktes orthogonal sind, welches mit Hilfe eines Integrals
definiert sind.
Solche Funktionenfamilien treten jedoch auch als Lösungen von
Differentialgleichungen.
Besonders interessant wird die Situation, wenn die Funktionen 
Polynome sind.

%
% Skalarprodukt
%
\subsection{Skalarprodukt}
Der reelle Vektorraum $\mathbb{R}^n$ trägt das Skalarprodukt
\[
\langle\;\,,\;\rangle
\colon
\mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}
:
(x,y)\mapsto \langle x, y\rangle = \sum_{k=1}^n x_iy_k,
\]
welches viele interessante Anwendungen ermöglicht.
Eine orthonormierte Basis macht es zum Beispiel besonders leicht,
eine Zerlegung eines Vektors in dieser Basis zu finden.
In diesem Abschnitt soll zunächst an die Eigenschaften erinnert
werden, die zu einem nützlichen 

\subsubsection{Eigenschaften eines Skalarproduktes}
Das Skalarprodukt erlaubt auch, die Länge eines Vektors $v$
als $|v| = \sqrt{\langle v,v\rangle}$ zu definieren.
Dies funktioniert natürlich nur, wenn die Wurzel auch immer
definiert ist, d.~h.~das Skalarprodukt eines Vektors mit sich
selbst darf nicht negativ sein.
Dazu dient die folgende Definition.

\begin{definition}
Sei $V$ ein reeller Vektorraum.
Eine bilineare Abbildung
\[
\langle\;\,,\;\rangle
\colon
V\times V
\to
\mathbb{R}
:
(u,v) \mapsto \langle u,v\rangle.
\]
heisst {\em positiv definit}, wenn für alle Vektoren $v \in V$ mit
$v\ne 0 \Rightarrow \langle v,v\rangle > 0$ 
Die {\em Norm} eines Vektors $v$ ist
$|v|=\sqrt{\langle v,v\rangle}$.
\end{definition}

Damit man mit dem Skalarprodukt sinnvoll rechnen kann, ist ausserdem
erforderlich, dass es eine einfache Beziehung zwischen 
$\langle x,y\rangle$ und $\langle y,x\rangle$ gibt.

\begin{definition}
Ein {\em Skalarprodukt} auf einem reellen Vektorraum $V$ ist eine
positiv definite, symmetrische bilineare Abbildung
\[
\langle\;\,,\;\rangle
\colon
V\times V
\to
\mathbb{R}
:
(u,v) \mapsto \langle u,v\rangle.
\]
\end{definition}

Das Skalarprodukt $\langle u,v\rangle=u^tv$ auf dem Vektorraum 
$\mathbb{R}^n$ erfüllt die Definition ganz offensichtlich,
sie führt auf die Komponentendarstellung
\[
\langle u,v\rangle = u^tv = \sum_{k=1}^n u_iv_i.
\]
Weitere Skalarprodukte ergeben ergeben sich mit jeder symmetrischen,
positiv definiten Matrix $G$ und der Definition
$\langle u,v\rangle_G=u^tGv$.
Ein einfacher Spezialfall tritt auf, wenn $G$ eine Diagonalmatrix
$\operatorname{diag}(w_1,\dots,w_n)$
mit positiven Einträgen $w_i>0$ auf der Diagonalen ist.
In diesem Fall schreiben wir
\[
\langle u,v\rangle_w
=
u^t\operatorname{diag}(w_1,\dots,w_n)v
=
\sum_{k=1}^n u_iv_i\,w_i
\]
und nennen $\langle \;\,,\;\rangle_w$ das {\em gewichtete Skalarprodukt}
mit {\em Gewichten $w_i$}.

\subsubsection{Skalarprodukte auf Funktionenräumen}
Das Integral ermöglicht jetzt, ein Skalarprodukt auf dem reellen
Vektorraum der stetigen Funktionen auf einem Intervall zu definieren.

\begin{definition}
Sei $V$ der reelle Vektorraum $C([a,b])$ der reellwertigen, stetigen
Funktion auf dem Intervall $[a,b]$.
Dann ist 
\[
\langle\;\,,\;\rangle
\colon
C([a,b]) \times C([a,b]) \to \mathbb{R}
:
(f,g) \mapsto \langle f,g\rangle = \int_a^b f(x)g(x)\,dx.
\]
ein Skalarprodukt.
\end{definition}

Die Definition ist offensichtlich symmetrisch in $f$ und $g$ und
aus den Eigenschaften des Integrals ist klar, dass das Produkt
bilinear ist:
\begin{align*}
\langle \lambda_1 f_1+\lambda_2f_2,g\rangle
&=
\int_a^b (\lambda_1f_(x) +\lambda_2f_2(x))g(x)\,dx
=
\lambda_1\int_a^b f_1(x) g(x)\,dx
+
\lambda_2\int_a^b f_2(x) g(x)\,dx
\\
&=
\lambda_1\langle f_1,g\rangle
+
\lambda_2\langle f_2,g\rangle.
\end{align*}
Ausserdem ist es positiv definit, denn wenn $f(x_0) \ne 0$ ist,
dann gibt es wegen der Stetigkeit von $f$ eine Umgebung
$U=[x_0-\varepsilon,x_0+\varepsilon]$, derart, dass $|f(x)| > \frac12|f(x_0)|$
ist für alle $x\in U$.
Somit ist das Integral
\[
\langle f,f\rangle
=
\int_a^b |f(x)|^2\,dx
\ge
\int_{x_0-\varepsilon}^{x_0+\varepsilon} |f(x)|^2\,dx
\ge
\int_{x_0-\varepsilon}^{x_0+\varepsilon} \frac14|f(x_0)|^2\,dx
=
\frac{1}{4}|f(x_0)|^2\cdot 2\varepsilon
=
\frac{|f(x_0)|^2\varepsilon}{2}
>0,
\]
was beweist, dass $\langle\;,\;\rangle$ positiv definit und damit
ein Skalarprodukt ist.

Die Definition kann noch etwas verallgemeinert werden, indem 
die Funktionswerte nicht überall auf dem Definitionsbereich 
gleich gewichtet werden. 

\begin{definition}
Sei $w\colon [a,b]\to \mathbb{R}^+$ eine positive, stetige Funktion,
dann ist
\[
\langle\;\,,\;\rangle_w
\colon
C([a,b]) \times C([a,b]) \to \mathbb{R}
:
(f,g) \mapsto \langle f,g\rangle_w = \int_a^b f(x)g(x)\,w(x)\,dx.
\]
das {\em gewichtete Skalarprodukt} mit {\em Gewichtsfunktion $w(x)$}.
\end{definition}

\subsubsection{Gram-Schmidt-Orthonormalisierung}
In einem reellen Vektorraum $V$ mit Skalarprodukt $\langle\;\,,\;\rangle$
kann aus einer beleibigen Basis $b_1,\dots,b_n$ mit Hilfe des 
Gram-Schmidtschen Orthogonalisierungsverfahrens immer eine
orthonormierte Basis $\tilde{b}_1,\dots,\tilde{b}_n$ Basis
gewonnen werden.
Es stellt sicher, dass für alle $k\le n$ gilt
\[
\langle b_1,\dots,b_k\rangle
=
\langle \tilde{b}_1,\dots,\tilde{b}_k\rangle.
\]
Zur Vereinfachung der Formeln schreiben wir $v^0=v/|v|$ für einen zu
$v$ parallelen Einheitsvektor.
Die Vektoren $\tilde{b}_i$ können mit Hilfe der Formeln
\begin{align*}
\tilde{b}_1
&=
(b_1)^0
\\
\tilde{b}_2
&=
\bigl(
b_2
-
\langle \tilde{b}_1,b_2\rangle \tilde{b}_1
\bigr)^0
\\
\tilde{b}_3
&=
\bigl(
b_3
-
\langle \tilde{b}_1,b_3\rangle \tilde{b}_1
-
\langle \tilde{b}_2,b_3\rangle \tilde{b}_2
\bigr)^0
\\
&\;\vdots
\\
\tilde{b}_n
&=
\bigl(
b_n
-
\langle \tilde{b}_1,b_n\rangle \tilde{b}_1
-
\langle \tilde{b}_2,b_n\rangle \tilde{b}_2
-\dots
-
\langle \tilde{b}_{n-1},b_n\rangle \tilde{b}_{n-1}
\bigr)^0
\end{align*}
iterativ berechnet werden.
Dieses Verfahren lässt sich auch auf Funktionenräume anwenden.

Die Normierung ist nicht unbedingt nötig und manchmal unangenehm,
da die Norm unschöne Quadratwurzeln einführt.
Falls es genügt, eine orthogonale Basis zu finden, kann darauf
verzichtet werden, bei der Orthogonalisierung muss aber berücksichtigt
werden, dass die Vektoren $\tilde{b}_i$ jetzt nicht mehr Einheitslänge
haben.
Die Formeln
\begin{align*}
\tilde{b}_0
&=
b_0
\\
\tilde{b}_1
&=
b_1
-
\frac{\langle b_1,\tilde{b}_0\rangle}{\langle \tilde{b}_0,\tilde{b}_0\rangle}\tilde{b}_0
\\
\tilde{b}_2
&=
b_2
-
\frac{\langle b_2,\tilde{b}_0\rangle}{\langle \tilde{b}_0,\tilde{b}_0\rangle}\tilde{b}_0
-
\frac{\langle b_2,\tilde{b}_1\rangle}{\langle \tilde{b}_1,\tilde{b}_1\rangle}\tilde{b}_1
\\
&\;\vdots
\\
\tilde{b}_n
&=
b_n
-
\frac{\langle b_n,\tilde{b}_0\rangle}{\langle \tilde{b}_0,\tilde{b}_0\rangle}\tilde{b}_0
-
\frac{\langle b_n,\tilde{b}_1\rangle}{\langle \tilde{b}_1,\tilde{b}_1\rangle}\tilde{b}_1
-
\dots
-
\frac{\langle b_n,\tilde{b}_{n-1}\rangle}{\langle \tilde{b}_{n-1},\tilde{b}_{n-1}\rangle}\tilde{b}_{n-1}.
\end{align*}
berücksichtigen dies.

\subsubsection{Selbstadjungierte Operatoren und Eigenvektoren}
Symmetrische Matrizen spielen eine spezielle Rolle in der
endlichdimensionalen linearen Algebra, weil sie sich immer
mit einer orthonormierten Basis diagonalisieren lassen.
In der vorliegenden Situation undendlichdimensionaler Vektorräume
brauchen wir eine angepasste Definition.

\begin{definition}
Eine lineare Selbstabbildung $A\colon V\to V$
eines Vektorrraums mit Skalarprodukt
heisst {\em selbstadjungiert}, wenn für alle Vektoren $u,v\in V$
heisst $\langle Au,v\rangle = \langle u,Av\rangle$.
\end{definition}

Es ist wohlbekannt, dass Eigenvektoren einer symmetrischen Matrix
zu verschiedenen Eigenwerten orthogonal sind.
Der Beweis ist direkt übertragbar, wir halten das Resultat hier
für spätere Verwendung fest.

\begin{satz}
Sind $f$ und $g$ Eigenvektoren eines selbstadjungierten Operators $A$
zu verschiedenen Eigenwerten $\lambda$ und $\mu$, dann sind $f$ und $g$
orthogonal.
\end{satz}

\begin{proof}[Beweis]
Im vorliegenden Zusammenhang möchten wir die Eigenschaft nutzen,
dass Eigenfunktionen eines selbstadjungierten Operatores zu verschiedenen
Eigenwerten orthogonal sind.
Dazu seien $Df = \lambda f$ und $Dg=\mu g$ und wir rechnen
\begin{equation*}
\renewcommand{\arraycolsep}{2pt}
\begin{array}{rcccrl}
\langle Df,g\rangle &=& \langle \lambda f,g\rangle &=& \lambda\phantom{)}\langle f,g\rangle
&\multirow{2}{*}{\hspace{3pt}$\biggl\}\mathstrut-\mathstrut$}\\
=\langle f,Dg\rangle &=& \langle f,\mu g\rangle &=& \mu\phantom{)}\langle f,g\rangle&
\\[2pt]
\hline
         0           & &                        &=& (\lambda-\mu)\langle f,g\rangle&
\end{array}
\end{equation*}
Da $\lambda-\mu\ne 0$ ist, muss $\langle f,g\rangle=0$ sein.
\end{proof}

\begin{beispiel}
Sei $C^1([0,2\pi], \mathbb{C})=C^1(S^1,\mathbb{C})$
der Vektorraum der $2\pi$-periodischen differenzierbaren Funktionen mit
dem Skalarprodukt 
\[
\langle f,g\rangle
=
\frac{1}{2\pi}\int_0^{2\pi} \overline{f(t)}g(t)\,dt
\]
enthält die Funktionen $e_n(t) = e^{int}$.
Der Operator
\[
D=i\frac{d}{dt}
\]
ist selbstadjungiert, denn mit Hilfe von partieller Integration erhält man
\[
\langle Df,g\rangle
=
\frac{1}{2\pi}
\int_0^{2\pi}
\underbrace{
\overline{i\frac{df(t)}{dt}}
}_{\uparrow}
\underbrace{g(t)}_{\downarrow}
\,dt
=
\underbrace{
\frac{-i}{2\pi}
\biggl[
\overline{f(t)}g(t)
\biggr]_0^{2\pi}
}_{\displaystyle=0}
+
\frac{1}{2\pi}
\int_0^{2\pi}
\overline{f(t)}i\frac{dg(t)}{dt}
\,dt
=
\langle f,Dg\rangle
\]
unter Ausnützung der $2\pi$-Periodizität der Funktionen.

Die Funktionen $e_n(t)$ sind Eigenfunktionen des Operators $D$, denn
\[
De_n(t) = i\frac{d}{dt}e^{int} = -n e^{int} = -n e_n(t).
\]
Nach obigem Satz sind die Eigenfunktionen von $D$ orthogonal.
\end{beispiel}

Das Beispiel illustriert, dass orthogonale Funktionenfamilien
ein automatisches Nebenprodukt selbstadjungierter Operatoren sind.

%
% Besselfunktionen also orthogonale Funktionenfamilie
%
\subsection{Bessel-Funktionen als orthogonale Funktionenfamilie}
Auch die Besselfunktionen sind eine orthogonale Funktionenfamilie.
Sie sind Funktionen differenzierbaren Funktionen $f(r)$ für $r>0$
mit $f'(r)=0$ und für $r\to\infty$ nimmt $f(r)$ so schnell ab, dass
auch $rf(r)$ noch gegen $0$ strebt.
Das Skalarprodukt ist
\[
\langle f,g\rangle
=
\int_0^\infty r f(r) g(r)\,dr,
\]
als Operator verwenden wir
\[
A = \frac{d^2}{dr^2} + \frac{1}{r}\frac{d}{dr} + s(r),
\]
wobei $s(r)$ eine beliebige integrierbare Funktion sein kann.
Zunächst überprüfen wir, ob dieser Operator wirklich selbstadjungiert ist.
Dazu rechnen wir
\begin{align}
\langle Af,g\rangle
&=
\int_0^\infty
r\,\biggl(f''(r)+\frac1rf'(r)+s(r)f(r)\biggr) g(r)
\,dr
\notag
\\
&=
\int_0^\infty rf''(r)g(r)\,dr
+
\int_0^\infty f'(r)g(r)\,dr
+
\int_0^\infty s(r)f(r)g(r)\,dr.
\notag
\intertext{Der letzte Term ist symmetrisch in $f$ und $g$, daher
ändern wir daran weiter nichts.
Auf das erste Integral kann man partielle Integration anwenden und erhält}
&=
\biggl[rf'(r)g(r)\biggr]_0^\infty
-
\int_0^\infty f'(r)g(r) + rf'(r)g'(r)\,dr
+
\int_0^\infty f'(r)g(r)\,dr
+
\int_0^\infty s(r)f(r)g(r)\,dr.
\notag
\intertext{Der erste Term verschwindet wegen der Bedingungen an die
Funktionen $f$ und $g$.
Der erste Term im zweiten Integral hebt sich gegen das
zweite Integral weg.
Der letzte Term ist das Skalarprodukt von $f'$ und $g'$.
Somit ergibt sich
}
&=
-\langle f',g'\rangle
+
\int_0^\infty s(r) f(r)g(r)\,dr.
\label{buch:integrale:orthogonal:besselsa}
\end{align}
Vertauscht man die Rollen von $f$ und $g$, erhält man das Gleiche, da im
letzten Ausdruck~\eqref{buch:integrale:orthogonal:besselsa} die Funktionen
$f$ und $g$ symmetrische auftreten.
Damit ist gezeigt, dass der Operator $A$ selbstadjungiert ist.
Es folgt nun, dass Eigenvektoren des Operators $A$ automatisch
orthogonal sind.

Eigenfunktionen von $A$ sind aber Lösungen der Differentialgleichung
\[
\begin{aligned}
&&
Af&=\lambda f
\\
&\Rightarrow\qquad&
f''(r) +\frac1rf'(r) + s(r)f(r) &= \lambda f(r)
\\
&\Rightarrow\qquad&
r^2f''(r) +rf'(r)+ (-\lambda r^2+s(r)r^2)f(r) &= 0
\end{aligned}
\]
sind.

Durch die Wahl $s(r)=1$ wird der Operator $A$ zum Bessel-Operator
$B$ definiert in
\eqref{buch:differentialgleichungen:bessel-operator}.
Die Lösungen der Besselschen Differentialgleichung zu verschiedenen Werten
des Parameters müssen also orthogonal sein, insbesondere sind die
Besselfunktion $J_\nu(r)$ und $J_\mu(r)$ orthogonal wenn $\mu\ne\nu$ ist.

%
% Orthogonale Polynome
%
\subsection{Orthogonale Polynome
\label{buch:integral:subsection:orthogonale-polynome}}
Die Polynome $1,x,x^2,\dots,x^n$ bilden eine Basis des Vektorraums
der Polynome vom Grad $\le n$.
Bezüglich des Skalarproduktes
\[
\langle p,q\rangle
=
\int_{-1}^1 p(x)q(x)\,dx
\]
sind sie jedoch nicht orthogonal, denn es ist
\[
\langle x^i,x^j\rangle
=
\int_{-1}^1 x^{i+j}\,dx
=
\biggl[\frac{x^{i+j+1}}{i+j+1}\biggr]_{-1}^1
=
\begin{cases}
\displaystyle
\frac{2}{i+j+1}&\qquad\text{$i+j$ gerade}\\
              0&\qquad\text{$i+j$ ungerade}.
\end{cases}
\]
Wir können daher das Gram-Schmidtsche Orthonormalisierungsverfahren
anwenden, um eine orthogonale Basis von Polynomen zu finden, was
wir im Folgenden tun wollen.

% XXX Orthogonalisierungsproblem so formulieren, dass klar wird,
% XXX dass man ein "Normierungskriterium braucht.

Da wir auf die Normierung verzichten, brauchen wir ein anderes
Kriterium, welches die Polynome eindeutig festlegen kann.
Wir bezeichnen das Polynom vom Grad $n$, das bei diesem Prozess
entsteht, mit $P_n(x)$ und legen willkürlich aber traditionskonform
fest, dass $P_n(1)=1$ sein soll.

Das Skalarprodukt berechnet ein Integral eines Produktes von zwei
Polynomen über das symmetrische Interval $[-1,1]$.
Ist die eine gerade und die andere ungerade, dann ist das
Produkt eine ungerade Funktion und das Skalarprodukt verschwindet.
Sind beide Funktionen gerade oder ungerade, dann ist das Produkt
gerade und das Skalarprodukt ist im Allgmeinen von $0$ verschieden.
Dies zeigt, dass es tatsächlich etwas zu Orthogonalisieren gibt.

Die ersten beiden Funktionen sind das konstante Polynom $1$ und
das Polynome $x$.
Nach obiger Beobachtung ist das Skalarprodukt $\langle 1,x\rangle=0$,
also ist $P_1(x)=x$.
Die Graphen der entstehenden Polynome sind in
Abbildung~\ref{buch:integral:orthogonal:legendregraphen}
dargestellt.
\begin{figure}
\centering
\includegraphics{chapters/060-integral/images/legendre.pdf}
\caption{Graphen der Legendre-Polynome $P_n(x)$ für $n=1,\dots,10$.
\label{buch:integral:orthogonal:legendregraphen}}
\end{figure}

\begin{lemma}
Die Polynome $P_{2n}(x)$ sind gerade, die Polynome $P_{2n+1}(x)$ sind
ungerade Funktionen von $x$.
\end{lemma}

\begin{proof}[Beweis]
Wir verwenden vollständige Induktion nach $n$.
Wir wissen bereits, dass $P_0(x)=1$ und $P_1(x)=x$ die verlangten
Symmetrieeigenschaften haben.
Im Sinne der Induktionsannahme nehmen wir daher an, dass die
Symmetrieeigenschaften für $P_k(x)$, $k<n$, bereits bewiesen sind.
$P_n(x)$ entsteht jetzt durch Orthogonalisierung nach der Formel
\[
P_n(x)
=
x^n
-
\langle P_{n-1},x^n\rangle P_{n-1}(x)
-
\langle P_{n-2},x^n\rangle P_{n-2}(x)
-\dots-
\langle P_1,x^n\rangle P_1(x)
-
\langle P_0,x^n\rangle P_0(x).
\]
Die Skalarprodukte
$\langle P_{n-1},x^n\rangle$,
$\langle P_{n-3},x^n\rangle$, $\dots$ verschwinden alle, so dass
$P_n(x)$ eine Linearkombination der Funktionen $x^n$, $P_{n-2}(x)$,
$P_{n-4}(x)$ ist, die die gleiche Parität wie $x^n$ haben.
Also hat auch $P_n(x)$ die gleiche Parität, was das Lemma beweist.
\end{proof}

Die Ortogonalisierung von $x^2$ liefert daher
\[
p(x) = x^2
-
\frac{\langle x^2,P_0\rangle}{\langle P_0,P_0\rangle} P_0(x)
=
x^2 - \frac{\int_{-1}^1x^2\,dx}{\int_{-1}^11\,dx}
=
x^2 - \frac{\frac{2}{3}}{2}=x^2-\frac13
\]
Dieses Polynom erfüllt die Standardisierungsbedingung noch 
nicht den $p(1)=\frac23$.
Daraus leiten wir ab, dass
\[
P_2(x) = \frac12(3x^2-1)
\]
ist.

Für $P_3(x)$ brauchen wir nur die Skalaprodukte
\[
\left.
\begin{aligned}
\langle x^3,P_1\rangle
&=
\int_{-1}^1  x^3\cdot x\,dx
=
\biggl[\frac15x^5\biggr]_{-1}^1
=
\frac25
\qquad
\\
\langle P_1,P_1\rangle
&=
\int_{-1}^1 x^2\,dx
=
\frac23
\end{aligned}
\right\}
\qquad
\Rightarrow
\qquad
p(x) = x^3 - \frac{\frac25}{\frac23}x=x^3-\frac{3}{5}x
\]
Die richtige Standardisierung ergibt sich,
indem man durch $p(1)=\frac25$ dividiert, also
\[
P_2(x) = \frac12(5x^3-3x).
\]

Die Berechnung weiterer Polynome verlangt, dass Skalarprodukte
$\langle x^n,P_k\rangle$ berechnet werden müssen, was wegen
der zunehmend komplizierten Form von $P_k$ etwas mühsam ist.
Wir berechnen den Fall $P_4$.
Dazu muss das Polynom $x^4$ um eine Linearkombination von
$P_2$ und $P_0(x)=1$ korrigiert werden.
Die Skalarprodukte sind
\begin{align*}
\langle x^4, P_0\rangle
&=
\int_{-1}^1 x^4\,dx = \frac25
\\
\langle P_0,P_0\rangle
&=
\int_{-1}^1 \,dx = 2
\\
\langle x^4,P_2\rangle
&=
\int_{-1}^1 \frac32x^6-\frac12 x^4\,dx
=
\biggl[\frac{3}{14}x^7-\frac{1}{10}x^5\biggr]_{-1}^1
=
\frac6{14}-\frac15
=
\frac8{35}
\\
\langle P_2,P_2\rangle
&=
\int_{-1}^1 \frac14(3x^2-1)^2\,dx
=
\int_{-1}^1 \frac14(9x^4-6x^2+1)\,dx
=
\frac14(\frac{18}{5}-4+2)
=\frac25.
\end{align*}
Daraus folgt für $p(x)$
\begin{align*}
p(x)
&=
x^4
-
\frac{\langle x^4,P_2\rangle}{\langle P_2,P_2\rangle}P_2(x)
-
\frac{\langle x^4,P_0\rangle}{\langle P_0,P_0\rangle}P_0(x)
\\
&=
x^4
-\frac47 P_2(x) - \frac15 P_0(x)
\\
&=
x^4 - \frac{6}{7}x^2 + \frac{3}{35}
\end{align*}
mit $p(1)=\frac{8}{35}$, so dass man
\[
P_4(x) =
\frac18(35x^4-30x^2+3)
\]
setzen muss.

\begin{figure}
\centering
\includegraphics{chapters/060-integral/images/orthogonal.pdf}
\caption{Orthogonalität der Legendre-Polynome $P_4(x)$ ({\color{blue}blau})
und $P_7(x)$ ({\color{darkgreen}grün}).
Die blaue Fläche ist die Fläche unter dem Graphen 
von $P_4(x)^2$, $P_4(x)$ muss durch die Wurzel aus diesem Flächeninhalt
geteilt werden, um ein Polynome mit Norm $1$ zu erhalten.
Für die grüne Fläche ist es $P_7(x)$.
Die rote Kurve ist der Graph der Funktion $P_4(x)\cdot P_7(x)$,
die rote Fläche ist deren Integral, sie ist $0$, d.~h.~die beiden
Funktionen sind orthogonal.
\label{buch:integral:orthogonal:legendreortho}}
\end{figure}

\begin{table}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|>{$}c<{$}|>{$}l<{$}|}
\hline
n&P_n(x)\\
\hline
 0&1
\\
 1&x
\\
 2&\frac12(3x^2-1)
\\
 3&\frac12(5x^3-3x)
\\
 4&\frac18(35x^4-30x^2+3)
\\
 5&\frac18(63x^5-70x^3+15x)
\\
 6&\frac1{16}(231x^6-315x^4+105x^2-5)
\\
 7&\frac1{16}(429x^7-693x^5+315x^3-35x)
\\
 8&\frac1{128}(6435x^8-12012x^6+6930x^4-1260x^2+35)
\\
 9&\frac1{128}(12155x^9-25740x^7+18018x^5-4620x^3+315x)
\\
10&\frac1{256}(46189x^{10}-109395x^8+90090x^6-30030x^4+3465x^2-63)
\\[2pt]
\hline
\end{tabular}
\caption{Die Legendre-Polynome $P_n(x)$ für $n=0,1,\dots,10$ sind
orthogonale Polynome vom Grad $n$, die den Wert $P_n(1)=1$ haben.
\label{buch:integral:table:legendre-polynome}}
\end{table}



Die so konstruierten Polynome heissen die {\em Legendre-Polynome}.
Durch weitere Durchführung des Verfahrens liefert die Polynome in
Tabelle~\ref{buch:integral:table:legendre-polynome}.
Die Graphen sind in Abbildung~\ref{buch:integral:orthogonal:legendregraphen}
dargestellt.
Abbildung~\ref{buch:integral:orthogonal:legendreortho} illustriert, 
dass die die beiden Polynome $P_4(x)$ und $P_7(x)$ orthogonal sind.
Das Produkt $P_4(x)\cdot P_7(x)$ hat Integral $=0$.

\input{chapters/060-integral/jacobi.tex}

\subsection{TODO}
\begin{itemize}
\item Jacobi-Polynome
\item Tschebyscheff-Polynome
\end{itemize}

%%
%% Differentialgleichungen
%%
%\subsection{Orthogonale Polynome und Differentialgleichungen}
%\subsubsection{Legendre-Differentialgleichung}
%\subsubsection{Legendre-Polyome}
%\subsubsection{Legendre-Funktionen zweiter Art}
%Siehe Wikipedia-Artikel \url{https://de.wikipedia.org/wiki/Legendre-Polynom}

\input{chapters/060-integral/legendredgl.tex}
\input{chapters/060-integral/sturm.tex}
\input{chapters/060-integral/gaussquadratur.tex}

